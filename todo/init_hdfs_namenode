- name: set all hdfs nodes
  hosts: HDFS-NN
  user: root
  gather_facts: yes
  tasks:
    - include_role:
        name: hdfs-namenode
      vars:
        group: HDFS-NN
        extra_group:
          - HDFS-JN
          - HDFS-ZK
          - HDFS-DN
        datanode_group: HDFS-DN
        hadoop_pkg: 'http://mirrors.ustc.edu.cn/apache/hadoop/core/hadoop-3.3.0/hadoop-3.3.0.tar.gz'
        http_auth_secret: '/data/hadoop-http-auth-signature-secret'
        core_site:
          fs.defaultFS: hdfs://mycluster
          hadoop.tmp.dir: /data/hadoopData/tmp
          fs.trash.interval: 4320
          ha.zookeeper.quorum: hdfs-zookeeper-1,hdfs-zookeeper-2,hdfs-zookeeper-3
          hadoop.http.filter.initializers: org.apache.hadoop.security.AuthenticationFilterInitializer
          hadoop.http.authentication.type: simple
          hadoop.http.authentication.signature.secret.file: /data/hadoop-3.3.0/share/hadoop-http-auth-signature-secret 
        hdfs_site:
          dfs.nameservices: mycluster
          dfs.ha.namenodes.mycluster: nn1,nn2
          dfs.ha.automatic-failover.enabled: "true"
          dfs.ha.fencing.methods: sshfence
          dfs.ha.fencing.ssh.private-key-files: /home/hdfs/.ssh/id_rsa  
          dfs.namenode.rpc-address.mycluster.nn1: hdfs-namenode-1:8020
          dfs.namenode.rpc-address.mycluster.nn2: hdfs-namenode-2:8020
          dfs.namenode.http-address.mycluster.nn1: hdfs-namenode-1:9870
          dfs.namenode.http-address.mycluster.nn2: hdfs-namenode-2:9870
          dfs.client.failover.proxy.provider.mycluster: org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
          dfs.namenode.shared.edits.dir: qjournal://hdfs-journalnode-1:8485;hdfs-journalnode-2:8485;hdfs-journalnode-3:8485/mycluster
          dfs.webhdfs.enabled: "true"
          dfs.namenode.name.dir: /data/hadoopData/dfs/name 
          dfs.replication: 3
          dfs.permissions.superusergroup: staff
          dfs.permissions.enabled: "false"
          dfs.permissions.ContentSummary.subAccess: "true"
